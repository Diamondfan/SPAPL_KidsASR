# Model Argument
model_name_or_path: "facebook/wav2vec2-large-lv60"
cache_dir: "cached_ctc_models/"
vocab_path: "conf/vocab.json"
feature_extractor_type: "wav2vec"
freeze_feature_encoder: True
apply_spec_augment: True
mask_time_prob: 0.30
mask_time_length: 10
mask_feature_prob: 0.10
mask_feature_length: 64
layerdrop: 0.0
activation_dropout: 0.1
ctc_loss_reduction: "mean"

# Data Argument
train_data_path:
  1:
    name: 'train'
    scp_path: data/train/wav.scp
    text_label: data/train/text

dev_data_path:
  1:
    name: 'development'
    scp_path: data/dev/wav.scp 
    text_label: data/dev/text

streaming: True
audio_column_name: "audio"
text_column_name: "sentence"
max_duration_in_seconds: 30
min_duration_in_seconds: 0
normalize_file: "conf/normalizer.json"
use_speed_perturb: False
sp_low: 0.9
sp_high: 1.1
use_pitch_perturb: False
pitch_level: 12  # 12 steps for octave 

# PEFT Argument
peft_type: #"adapter"
lora_dim: 8
lora_alpha: 128
dropout: 0.1
bottleneck_dim: 32 #16
to_encoder: True
peft_encoder_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
to_decoder: True #False
peft_decoder_layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
prompt_n_tokens: [100, 20]  # [encoder, decoder]
prompt_init_vocab: True
prompt_random_range: 0.5
prefix_seq_len: [50, 10]
prefix_n_layer: 12
prefix_dropout_rate: 0.0
prefix_hidden_dim: 16

# Train Argument
seed: 1234
max_steps: 12000
output_dir: "exp/wav2vec2_large_fullfinetuning_lr3e-4_2gpus_bth8_grad2_12ksteps_warm1k/"
overwrite_output_dir: False
per_device_train_batch_size: 8  # actually for all gpus because using split_batches=True
gradient_accumulation_steps: 2
per_device_eval_batch_size: 8 
logging_steps: 50
learning_rate: 0.0003    # 1e-3 for peft and 1e-5 for full finetuning
warmup_steps: 1000
evaluation_strategy: "steps"
eval_steps: 3000
save_strategy: "steps"
save_steps: 3000 
length_column_name: "input_length"
gradient_checkpointing: True
group_by_length: False              # True for map style Dataset
fp16: True
dataloader_drop_last: True
